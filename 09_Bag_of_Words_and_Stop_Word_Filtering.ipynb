{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 9 Mengenal Text Processing: <br/>Bag of Words & Stop Word Filtering\n",
    "\n",
    "Source : [Indonesia Belajar](https://www.youtube.com/watch?v=U30sF4m0bd0&list=PL2O3HdJI4voHNEv59SdXKRQVRZAFmwN9E&index=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of Words model sebagai representasi text\n",
    "\n",
    "Bag of Words (BoW) adalah metode representasi teks yang sederhana. Metode ini mengabaikan tata bahasa dan urutan kata dalam kalimat. Dalam BoW, teks akan dikonversi menjadi huruf kecil dan tanda baca akan diabaikan.\n",
    "\n",
    "Referensi: [https://en.wikipedia.org/wiki/Bag-of-words_model](https://en.wikipedia.org/wiki/Bag-of-words_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daftar kalimat yang akan digunakan sebagai corpus\n",
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',  # Menginformasikan keberadaan Linux sejak pertengahan tahun 1990-an\n",
    "    'Linux distributions include the Linux kernel.',  # Menyebutkan bahwa distribusi Linux mencakup kernel Linux\n",
    "    'Linux is one of the most prominent open-source software.'  \n",
    "    # Menyatakan bahwa Linux adalah salah satu perangkat lunak open-source yang paling menonjol\n",
    "]\n",
    "\n",
    "# Menampilkan corpus yang telah didefinisikan\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag of Words model dengan `CountVectorizer`\n",
    "\n",
    "Bag of Words model dapat diterapkan dengan memanfatkan `CountVectorizer`.\n",
    "\n",
    "Cara Kerja `CountVectorizer`:\n",
    "- Tokenisasi : Memecah teks menjadi kata-kata (token) individual.\n",
    "- Pembersihan Teks : Mengubah semua teks menjadi huruf kecil dan menghapus tanda baca.\n",
    "- Membangun Kosakata : Membuat daftar kata-kata unik yang ditemukan dalam seluruh corpus.\n",
    "- Membuat Vektor Film : Menghitung frekuensi kemunculan setiap kata dalam setiap dokumen dan membentuk matriks fitur.\n",
    "\n",
    "Fitur Utama `CountVectorizer`:\n",
    "- Lowercasing : CountVectorizer mengonversi semua teks menjadi huruf kecil.\n",
    "- Stop Words : Dapat diatur untuk menghapus kata-kata umum yang tidak memberikan informasi penting\n",
    "- N-grams : Dapat dikonfigurasi untuk mempertimbangkan kombinasi kata.\n",
    "- Binary Mode : Dapat diatur untuk hanya menunjukkan keberadaan atau ketidakhadiran kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import CountVectorizer dari pustaka scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Inisialisasi CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Menerapkan fit_transform pada corpus untuk tokenisasi dan menghitung frekuensi kata\n",
    "# Mengonversi hasilnya menjadi matriks padat (dense matrix)\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "# Menampilkan matriks fitur yang dihasilkan\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1990s', 'around', 'been', 'distributions', 'has', 'include', 'is',\n",
       "       'kernel', 'linux', 'mid', 'most', 'of', 'one', 'open', 'prominent',\n",
       "       'since', 'software', 'source', 'the'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mendapatkan daftar fitur (kata-kata unik) yang dihasilkan dari proses tokenisasi\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Euclidean Distance untuk mengukur kedekatan/jarak antar dokumen (vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2: [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3: [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3: [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "# Import euclidean_distances dari pustaka sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Looping untuk menghitung jarak antara setiap pasangan dokumen menggunakan Euclidean distance\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        # Menghindari perhitungan jarak antara dokumen yang sama\n",
    "        if i == j:\n",
    "            continue\n",
    "        # Mengonversi matriks vectorized_X menjadi tipe data numpy array\n",
    "        X_i = np.asarray(vectorized_X[i])\n",
    "        X_j = np.asarray(vectorized_X[j])\n",
    "        # Menghitung jarak antara dua dokumen menggunakan Euclidean distance\n",
    "        jarak = euclidean_distances(X_i, X_j)\n",
    "        # Menampilkan hasil jarak antara dua dokumen\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}: {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stop Word Filtering pada text\n",
    "\n",
    "Stop Word Filtering menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an),  auxiliary verbs (do, be, will), dan prepositions (on, in, at).\n",
    "\n",
    "Referensi: [https://en.wikipedia.org/wiki/Stop_word](https://en.wikipedia.org/wiki/Stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stop Word Filtering dengan `CountVectorizer`\n",
    "\n",
    "Stop Word Filtering juga dapat diterapkan dengan memanfatkan `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import CountVectorizer dari pustaka scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Inisialisasi CountVectorizer dengan mengatur stop words ke 'english'\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Menerapkan fit_transform pada corpus untuk tokenisasi dan menghitung frekuensi kata\n",
    "# Mengonversi hasilnya menjadi matriks padat (dense matrix)\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "# Menampilkan matriks fitur yang dihasilkan\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1990s', 'distributions', 'include', 'kernel', 'linux', 'mid',\n",
       "       'open', 'prominent', 'software', 'source'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mendapatkan daftar fitur (kata-kata unik) yang dihasilkan dari proses tokenisasi\n",
    "vectorizer.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
